# -*- coding: utf-8 -*-
"""Assignment 3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sNi1IW7cu3NBLRZ89abOcr-lYTeC_Wwe

First, we will install the important modules needed for the assignment.
"""

!pip install scikeras

!pip install --upgrade scikit-learn

!pip install dill

"""Then, we can import all the important modules and functions needed for the assignment."""

# For data preprocessing
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder

# For feature engineering
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE

# For exploratory data analysis
import matplotlib.pyplot as plt
import seaborn as sns

# For model training
from sklearn.model_selection import train_test_split

# For model optimization
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from scikeras.wrappers import KerasClassifier
from tensorflow.keras.optimizers import Adam, SGD
from keras.models import Sequential
from keras.callbacks import EarlyStopping
from keras.layers import BatchNormalization

# For model testing
from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report

# For saving the model
from keras.models import load_model, save_model
import dill as pickle
from google.colab import files
import joblib

"""Next, we will import the needed datasets for the assignment."""

# Import dataset
churn_df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Intro to AI/datasets_13996_18858_WA_Fn-UseC_-Telco-Customer-Churn.csv")

"""Now, we can start data preprocessing.

# <center>__Data Preprocessing__</center>

Firstly, we will examine the dataset to have a general idea of how the features look like. This will help us know which columns to remove, impute, and encode.
"""

# Examining the dataset
churn_df.head()

"""Per observation, we will remove the customerID column because it serves as a unique identifier for each customer and doesn't provide any meaningful information or predictive value for the machine learning task at hand."""

# Removing customerID column
churn_df.drop('customerID', axis=1, inplace=True)

# Examining the dataset
churn_df.info(verbose=True)

"""Per observation, we see that none of the columns contain null values, and most of them are object data types. We can also observe that although the TotalCharges contains float values, it is stored as an object data type.

We will convert the TotalCharges from object to float.
"""

# Converting TotalCharges column
churn_df['TotalCharges'] = pd.to_numeric(churn_df['TotalCharges'], errors='coerce')

# Examining the new dataset
churn_df.info(verbose=True)

"""Now, we need to separate the dataset into numeric and non-numeric columns for imputation and encoding."""

# Separating the numeric columns from the dataframe
numeric_columns = churn_df.select_dtypes(include=['number'])

# Imputing the missing values with the mean value
imp=SimpleImputer()
imp.fit(numeric_columns)
imputed_data=imp.fit_transform(numeric_columns)
numeric_columns=pd.DataFrame(imputed_data, columns=numeric_columns.columns)

# Examining the numeric columns
numeric_columns.info()

"""Now that we have imputed missing values and the numeric dataframe is complete, we can move on to the non-numeric columns."""

# Separating the string categorical values from the dataframe
categorical_columns = churn_df.select_dtypes(exclude=['number'])

# Create a dictionary to store LabelEncoders
label_encoders = {}

# Encoding the object values to numeric data types
for col in categorical_columns.columns:
    le = LabelEncoder()
    categorical_columns[col] = le.fit_transform(categorical_columns[col])

    # Save the LabelEncoder in the dictionary
    label_encoders[col] = le

# Save the dictionary of LabelEncoders
with open('label_encoders.pkl', 'wb') as file:
    pickle.dump(label_encoders, file)

"""After imputing and encoding the numerical and non-numerical data types, we will combine them and check their importance with the target variable using a classifier model. This is where the feature engineering process starts."""

# Combining the two dataframes
combined_df = pd.concat([numeric_columns, categorical_columns], axis=1)
combined_df.head()

"""Now that the data has been cleaned and processed, we can move onto the feature engineering process where we will select important features that will help us predict the churn rate.

# <center>__Feature Engineering, Extraction and Select Features__</center>

For the feature engineering, extraction, and selection process. We will use Recursive Feature Elimination. RFE is a recursive process that starts with all the features in the dataset and then iteratively removes the least essential features until the desired number of features is reached.

Before we start, we need to separate our feature and target variables, and create our classifier object.
"""

# Target variable
y = combined_df['Churn']

# Feature variables
X = combined_df.drop('Churn', axis=1)

# Classifier object
rfc = RandomForestClassifier()

"""Then, we will scale these features to ensure that they have consistent scales and to prevent features with larger magnitudes from dominating the modeling process."""

# Create a dictionary to store StandardScaler instances
scalers = {}

for feature in X.columns:
    # Scaling the feature variables
    sc = StandardScaler()
    scaled_data = sc.fit_transform(X[[feature]])

    # Save the StandardScaler instance to the dictionary
    scalers[feature] = sc

    # Transform the feature in the original DataFrame
    X[feature] = scaled_data

# Save the dictionary of StandardScalers
with open('scalers.pkl', 'wb') as file:
    pickle.dump(scalers, file)

# Now, X contains the scaled features
X.head()

"""Next, we'll use RFE find the optimal number of features."""

# Use RFE to find the optimal number of features
selector = RFE(rfc)
selector = selector.fit(X, y)

# Print the optimal number of features
print("Optimal number of features: %d" % selector.n_features_)

# Get the selected features
selected_features = X.columns[selector.support_]

# Print the selected features
print("Selected features: %s" % selected_features)

# Select only the relevant features
X_rfe = combined_df[selected_features]
X_rfe.head()

"""Now we will compare the relevant features with the target variable using Explorative Data Analysis.

# <center>__Exploratory Data Analysis__</center>
"""

churn_df.head()

"""We will use bar charts to compare churn rates with categorical variables and histograms to compare churn rates with numerical variables"""

# Determine categorical columns
categorical_columns = churn_df[selected_features].select_dtypes(exclude=['number'])

# Create bar charts for each categorical column
for category in categorical_columns:
    plt.figure(figsize=(8, 6))
    sns.countplot(data=churn_df, x=category, hue='Churn')
    plt.title(f'Churn Count for {category}')
    plt.xlabel(category)
    plt.ylabel('Count')
    plt.legend(title='Churn', labels=['No', 'Yes'])
    plt.show()

"""Per observation, the following conclusions can be made:

*   Gender: There is an equally high distribution (around 2,500 people) of males and females that do not churn and an equally low distribution (around 1,000 people) that don't churn for both genders.

*   Partner: People without a partner are more likely to churn (around 1,200 people) and people with a partner are less likely to churn (more than 2,500 people).

*   MultipleLines: Majority of people with no MultipleLines do not churn (aroung 2,500 people). However, more people that churn either have MultipleLines or not.

*   InternetService: People who use DSL are the least likely to churn (around 2,000 people). People who use Fibre optic are the most likely to churn (around 1,250 people).




"""

# Get a list of all numeric columns (excluding 'Churn')
numeric_columns_eda = churn_df[selected_features].select_dtypes(include=['number'])

# Create violin plots for each numeric column
for column in numeric_columns_eda:
    plt.figure(figsize=(8, 6))
    sns.violinplot(data=churn_df, x='Churn', y=column, hue='Churn', split=True)
    plt.title(f'Distribution of {column} by Churn')
    plt.xlabel('Churn')
    plt.ylabel(column)
    plt.legend(title='Churn', labels=['No', 'Yes'])
    plt.show()

"""The Exploratory Data Analysis reveals that the following customer profiles are more likely to experience high rates of churning:

- Customers who do not have a partner.
- Customers who have Fiber Optic internet service.
- Customers who do not have online security.
- Customers who do not have online backup.
- Customers who do not have device backup.
- Customers who do not have tech support.
- Customers with a Month-to-Month contract.
- Customers with paperless billing.
- Customers who use Electronic Check as a payment method.
- Customers with a short tenure.
- Customers who pay a monthly charge of $80.

Now that we have defined the customer profile, we can move on to model training, testing, and optimization.

# <center>__Model Training, Testing, and Optimization__</center>

Using cross validation and GridSearchCV, we will train and evaluate a multi-layer perceptron model with different hyperparameters, such as the number of hidden layers, the number of neurons, the activation function, the learning rate, and the regularization parameter, and measure its accuracy and calculate the AUC score.
"""

# Select only the relevant features
X_rf = X[selected_features]

# Saving the scaled features
#X_rf.to_csv('training_data.csv')
#files.download('training_data.csv')

# Split the dataset into training (80%) and temp (20%)
X_train, X_temp, Y_train, Y_temp = train_test_split(X_rf, y, test_size=0.2, random_state=42)

# Split the temp set into testing (50%) and validation (50%)
X_test, X_val, Y_test, Y_val = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)

# Keras Functional API model
input_layer = Input(shape=(X_train.shape[1],))
hidden_layer_1 = Dense(32, activation='relu')(input_layer)
hidden_layer_2 = Dense(24, activation='relu')(hidden_layer_1)
hidden_layer_3 = Dense(12, activation='relu')(hidden_layer_2)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

result = model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_val, Y_val))

# Evaluating the model's train loss and accuracy
loss, accuracy = model.evaluate(X_train, Y_train)
print(f'Train Loss: {loss:.2f}')
print(f'Train Accuracy: {accuracy*100:.2f}')

# Evaluating the model's test loss and accuracy
loss, accuracy = model.evaluate(X_test, Y_test)
print(f'Test Loss: {loss:.2f}')
print(f'Test Accuracy: {accuracy*100:.2f}')

# Predict probabilities on the test set
y_pred_prob = model.predict(X_test)

# Flatten the predictions if needed
y_pred_prob = np.ravel(y_pred_prob)

# Calculate AUC score
auc_score = roc_auc_score(Y_test, y_pred_prob)

# Round probabilities to obtain binary predictions
y_pred_binary = np.round(y_pred_prob)

# Calculate accuracy
accuracy = accuracy_score(Y_test, y_pred_binary)

# Print or use the accuracy and AUC score as needed
print(f"Accuracy: {accuracy*100:.2f}")
print(f"AUC Score: {auc_score:.2f}")

"""Per observation, the model gave an accuracy score of 80.54% with an AUC score of 0.85. This is good, but it could be further improved. We will now add hyperparameter testing to try and improve the model's performance."""

# Define the Keras model as a function
def create_model(activation='relu', hidden_layer_sizes=(64, 32), alpha=0.0001, dropout_rate=0.0):
    inputs = Input(shape=(X_train.shape[1],))

    # Input layer with BatchNormalization
    x = BatchNormalization()(inputs)

    # Increase the size of the first hidden layer
    x = Dense(hidden_layer_sizes[0] * 2, activation=activation)(x)
    x = Dropout(dropout_rate)(x)

    for units in hidden_layer_sizes[1:]:
        # Use a smaller dropout rate for subsequent layers
        x = Dense(units, activation=activation)(x)
        x = Dropout(dropout_rate * 0.8)(x)

    outputs = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=inputs, outputs=outputs)
    optimizer = Adam(learning_rate=0.0005)

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    return model

# Create KerasClassifier
model = KerasClassifier(build_fn=create_model,
                        activation='relu',
                        epochs=100,
                        batch_size=32,
                        verbose=1,
                        alpha=0.0001,
                        hidden_layer_sizes=(64, 32),
                        dropout_rate=0.2
                        )

# Define hyperparameter grid
PARAMETERS = {
    'activation': ['relu', 'tanh'],
    'hidden_layer_sizes': [(64, 32)],
    'alpha': [0.0001],
    'dropout_rate': [0.2],
}

# Instantiate GridSearchCV
gs_model = GridSearchCV(estimator=model, param_grid=PARAMETERS, scoring='accuracy', cv=3, verbose=1)

# Fit the model with early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
gs_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), callbacks=[early_stopping])

# Summarize results
print("Best: %f using %s" % (gs_model.best_score_, gs_model.best_params_))
means = gs_model.cv_results_['mean_test_score']
stds = gs_model.cv_results_['std_test_score']
params = gs_model.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

best_model = gs_model.best_estimator_

# Predict probabilities on the test set
y_pred_prob = best_model.predict(X_test)

# Flatten the predictions if needed
y_pred_prob = np.ravel(y_pred_prob)

# Calculate AUC score
auc_score = roc_auc_score(Y_test, y_pred_prob)

# Round probabilities to obtain binary predictions
y_pred_binary = np.round(y_pred_prob)

# Calculate accuracy
accuracy = accuracy_score(Y_test, y_pred_binary)

# Print or use the accuracy and AUC score as needed
print(f"Accuracy: {accuracy*100:.2f}")
print(f"AUC Score: {auc_score:.2f}")

# Showing the confusion matrix
cm = confusion_matrix(Y_test, y_pred_prob)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Showing the classification report
report = classification_report(Y_test, y_pred_prob)
print(report)

"""# <center>__Model Deployment__</center>"""

best_model = create_model()

# Save using pickle
filename = 'best_model.joblib'
joblib.dump(best_model, filename)

# Saving the validation data
combined_val = pd.concat([pd.DataFrame(X_val), pd.DataFrame(Y_val)], axis=1)
values_df = pd.DataFrame(combined_val)

# Changing the dataframe to a csv file to load
values_df.to_csv('validation_data.csv',index = False)